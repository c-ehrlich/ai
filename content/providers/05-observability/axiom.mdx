---
title: Axiom
description: Measure, observer, and improve your AI SDK application with Axiom
---
# Axiom Observability

**Axiom** is a data platform with specialized features for **AI engineering workflows**, helping you build sophisticated AI systems with confidence.

Axiom’s integration with the Vercel AI SDK uses a simple model wrapper to automatically capture detailed traces for every LLM call, giving you immediate visibility into your application's performance, cost, and behavior.

## Quickstart

This guide will get you sending Vercel AI SDK traces to Axiom in minutes.

### 1. Configure Axiom

First, you'll need an Axiom organization, a dataset to send traces to, and an API token.

  * [Create an Axiom organization](https://app.axiom.co/register).
  * [Create a new dataset](https://app.axiom.co/datasets) (e.g., `my-ai-app`).
  * [Create an API token](https://app.axiom.co/settings/api-tokens) with ingest permissions for your dataset.

### 2. Install the SDK

Install the Axiom package in your project:

<Tabs items={['pnpm', 'npm', 'yarn', 'bun']}>
  <Tab>
    <Snippet text="pnpm add axiom" />
  </Tab>
  <Tab>
    <Snippet text="npm install axiom" />
  </Tab>
  <Tab>
    <Snippet text="yarn add axiom" />
  </Tab>

  <Tab>
    <Snippet text="bun add axiom" />
  </Tab>
</Tabs>

### 3. Set Environment Variables

Configure your environment variables in a `.env` file. This uses the standard OpenTelemetry configuration to send traces directly to your Axiom dataset.

```bash
# .env

# Axiom Configuration
AXIOM_TOKEN="YOUR_AXIOM_API_TOKEN"
AXIOM_DATASET="your-axiom-dataset-name"

# Vercel and OpenTelemetry Configuration
OTEL_SERVICE_NAME="my-ai-app"
OTEL_EXPORTER_OTLP_ENDPOINT="https://api.axiom.co/v1/traces"
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer YOUR_AXIOM_API_TOKEN,X-Axiom-Dataset=your-axiom-dataset-name"

# Your AI Provider Key
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
```

Replace the placeholder values with your actual Axiom token and dataset name.

### 4. Wrap and Use the AI Model

In your application code, import `wrapAISDKModel` from Axiom and use it to wrap your existing AI SDK model client.

```typescript
import { createOpenAI } from '@ai-sdk/openai';
import { generateText } from 'ai';
import { wrapAISDKModel } from 'axiom/ai';

// 1. Create your standard AI model provider
const openaiProvider = createOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// 2. Wrap the model to enable automatic tracing
const tracedGpt4o = wrapAISDKModel(openaiProvider('gpt-4o'));

// 3. Use the wrapped model as you normally would
const { text } = await generateText({
  model: tracedGpt4o,
  prompt: 'What is the capital of Spain?',
});

console.log(text);
```

That's it\! Any calls made using the `tracedGpt4o` model will now automatically send detailed traces to your Axiom dataset.

## What You'll See in Axiom

Once integrated, your Axiom dataset will include:

  * **AI Trace Waterfall:** A dedicated view to visualize single and multi-step LLM workflows.
  * **Gen AI Dashboard:** A pre-built dashboard to monitor cost, latency, token usage, and error rates.
  * **Detailed Spans:** Rich telemetry for every call, including the full prompt and completion, token counts, and model information.

## Advanced Usage

Axiom’s AI SDK offers more advanced instrumentation for deeper visibility:

  * **Business Context:** Use the `withSpan` function to group LLM calls under a specific business capability (e.g., `customer_support_agent`).
  * **Tool Tracing:** Use the `wrapTool` helper to automatically trace the execution of tools your AI model calls.

To learn more about these features, see the [Axiom AI SDK Instrumentation guide](https://axiom.co/docs/ai-engineering/observe/axiom-ai-sdk-instrumentation).

## Resources

  * [Axiom AI Engineering Documentation](https://axiom.co/docs/ai-engineering/overview)
  * [Axiom AI SDK on GitHub](https://github.com/axiomhq/ai)
  * [Full Quickstart Guide](https://axiom.co/docs/ai-engineering/quickstart)